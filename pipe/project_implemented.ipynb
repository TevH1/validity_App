{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07649ec3-f8e7-4e44-a155-d1443143871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twh/miniconda3/envs/proj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# ── Sentiment extractor ──────────────────────────────────────────────────────\n",
    "class EmojiAndSentimentFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text = X[\"original_text\"].fillna(\"\")\n",
    "        emoji_count = text.apply(lambda t: sum(1 for c in t if c in __import__(\"emoji\").EMOJI_DATA))\n",
    "        vader_comp  = text.apply(lambda t: self.analyzer.polarity_scores(t)[\"compound\"])\n",
    "        return np.vstack([emoji_count, vader_comp]).T\n",
    "\n",
    "# ── Two‐stage emotion ─────────────────────────────────────────────────────────\n",
    "class EmotionPipeline:\n",
    "    def __init__(self, emo2_pkl, emo_pkl, emo_le_pkl):\n",
    "        self.emo2  = joblib.load(emo2_pkl)   # emotionality\n",
    "        self.emo   = joblib.load(emo_pkl)    # fine‐emotion\n",
    "        self.le    = joblib.load(emo_le_pkl) # label encoder\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xdf = X[[\"original_text\",\"english_keywords\",\"primary_theme\",\"sentiment\"]].fillna(\"\")\n",
    "        is_em = self.emo2.predict(Xdf)\n",
    "        out   = np.array([\"neutral\"]*len(Xdf), dtype=object)\n",
    "\n",
    "        mask  = (is_em == 1)\n",
    "        if mask.any():\n",
    "            sub = Xdf.loc[mask]\n",
    "            p   = self.emo.predict(sub)\n",
    "            out[mask] = self.le.inverse_transform(p)\n",
    "\n",
    "\n",
    "class ValidityPipeline:\n",
    "    def __init__(self, cfg):\n",
    "        # 1) Primary‐theme\n",
    "        self.prim_model = joblib.load(cfg[\"PRIM_MODEL_PKL\"])\n",
    "        self.prim_scl   = joblib.load(cfg[\"PRIM_SCALER_PKL\"])\n",
    "        self.prim_le    = joblib.load(cfg[\"PRIM_LE_PKL\"])      # <— load label encoder\n",
    "\n",
    "        # 2) Secondary‐theme\n",
    "        self.sec_models = joblib.load(cfg[\"SEC_MODELS_PKL\"])\n",
    "        self.sec_scl    = joblib.load(cfg[\"SEC_SCALER_PKL\"])\n",
    "\n",
    "        # 3) Sentiment\n",
    "        self.sent_pipe  = joblib.load(cfg[\"SENT_PKL\"])\n",
    "\n",
    "        # 4) Full emotion pipeline\n",
    "        full_emo_pipe   = joblib.load(cfg[\"FULL_EMO_PKL\"])\n",
    "        self.emo2_model = full_emo_pipe.emotionality_model\n",
    "        self.emo_model  = full_emo_pipe.emotion_model\n",
    "        self.emo_le     = joblib.load(cfg[\"EMO_LE_PKL\"])\n",
    "        self.emo_ohe    = OneHotEncoder(\n",
    "            categories=[list(self.emo_le.classes_)],\n",
    "            handle_unknown=\"ignore\",\n",
    "            sparse_output=False\n",
    "        )\n",
    "\n",
    "        # 5) Final validity classifier\n",
    "        self.final_clf  = joblib.load(cfg[\"FINAL_MODEL_OUT\"])\n",
    "\n",
    "    def predict_primary_theme(self, X_embed, stats):\n",
    "        \"\"\"Scale stats, stack with embeddings, then predict and decode.\"\"\"\n",
    "        stats_s = self.prim_scl.transform(stats)\n",
    "        Xp      = np.hstack([X_embed, stats_s])\n",
    "        proba   = self.prim_model.predict_proba(Xp)\n",
    "        idx     = np.argmax(proba, axis=1)\n",
    "        return self.prim_le.inverse_transform(idx)\n",
    "\n",
    "    def preprocess(self, texts, english_keywords=None):\n",
    "        n = len(texts)\n",
    "        ek = english_keywords or [\"\"]*n\n",
    "        df = pd.DataFrame({\n",
    "            \"original_text\":    texts,\n",
    "            \"english_keywords\": ek\n",
    "        })\n",
    "\n",
    "        # 1) get embeddings & text‐stats (exactly as before) …\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        X_embed = embedder.encode(\n",
    "            df[\"original_text\"] + \" \" + df[\"english_keywords\"],\n",
    "            batch_size=64, convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        df[\"combined\"]   = df[\"original_text\"] + \" \" + df[\"english_keywords\"]\n",
    "        df[\"kw_overlap\"] = df[\"combined\"].apply(\n",
    "            lambda t: len(set(re.findall(r\"\\b[a-z]{3,}\\b\", t.lower())))\n",
    "        )\n",
    "        df[\"txt_len\"] = df[\"original_text\"].str.len()\n",
    "        df[\"has_q\"]   = df[\"original_text\"].str.contains(r\"\\?\").astype(int)\n",
    "        df[\"caps\"]    = df[\"original_text\"].str.count(r\"[A-Z]\")\n",
    "        df[\"excl\"]    = df[\"original_text\"].str.count(r\"!\")\n",
    "\n",
    "        stats = df[[\"kw_overlap\",\"txt_len\",\"has_q\",\"caps\",\"excl\"]].values\n",
    "\n",
    "        # 2) PRIMARY THEME FIRST\n",
    "        df[\"primary_theme\"] = self.predict_primary_theme(X_embed, stats)\n",
    "\n",
    "        # 3) DUMMY main_emotion so sentiment pipeline will accept it\n",
    "        df[\"main_emotion\"] = \"\"    # or \"neutral\" — just must exist\n",
    "\n",
    "        # 4) NOW FILL SENTIMENT (requires 4 cols)\n",
    "        Xs = df[[\n",
    "            \"english_keywords\",\n",
    "            \"original_text\",\n",
    "            \"primary_theme\",\n",
    "            \"main_emotion\"\n",
    "        ]].fillna(\"\")\n",
    "        df[\"sentiment\"] = self.sent_pipe.predict(Xs).astype(float)\n",
    "\n",
    "        # 5) TRUE two‐stage EMOTION (overwrites dummy)\n",
    "        emot_flag = self.emo2_model.predict(\n",
    "            df[[\"original_text\",\"english_keywords\",\"primary_theme\",\"sentiment\"]]\n",
    "        )\n",
    "        df[\"main_emotion\"] = \"neutral\"\n",
    "        idxs = np.where(emot_flag == 1)[0]\n",
    "        if len(idxs):\n",
    "            sub   = df.iloc[idxs]\n",
    "            fine  = self.emo_model.predict(\n",
    "                sub[[\"original_text\",\"english_keywords\",\"primary_theme\",\"sentiment\"]]\n",
    "            )\n",
    "            df.loc[idxs, \"main_emotion\"] = self.emo_le.inverse_transform(fine)\n",
    "\n",
    "        emo_feat = self.emo_ohe.fit_transform(df[[\"main_emotion\"]])\n",
    "\n",
    "        # 6) SECONDARY THEME PROBS, FINAL STACK …\n",
    "        sec_stats   = np.hstack([stats, df[\"sentiment\"].values.reshape(-1,1)])\n",
    "        sec_stats_s = self.sec_scl.transform(sec_stats)\n",
    "\n",
    "        sec_probs = []\n",
    "        for m in self.sec_models:\n",
    "            sec_probs.append(\n",
    "                m.predict_proba(\n",
    "                    np.hstack([X_embed, sec_stats_s, emo_feat])\n",
    "                )[:,1]\n",
    "            )\n",
    "\n",
    "        X_final = np.hstack([\n",
    "            self.prim_model.predict_proba(\n",
    "                np.hstack([X_embed, self.prim_scl.transform(stats)])\n",
    "            ),\n",
    "            np.vstack(sec_probs).T,\n",
    "            emo_feat,\n",
    "            df[\"sentiment\"].values.reshape(-1,1)\n",
    "        ])\n",
    "\n",
    "        return X_final\n",
    "        return X_final\n",
    "    def predict(self, texts, english_keywords=None):\n",
    "        Xf     = self.preprocess(texts, english_keywords)\n",
    "        preds  = self.final_clf.predict(Xf)\n",
    "        probs  = self.final_clf.predict_proba(Xf)[:,1]\n",
    "        return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f838855-c312-4209-b6f9-2b79feff0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load(\"validity_full_pipeline.pkl\")  # now it can find ValidityPipeline, Embedder, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0303edff-8159-40f2-8d98-6dd1702627b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Load/construct the keyword_bank ────────────────────\n",
    "import pandas as pd\n",
    "\n",
    "KW_BANK_CSV  = \"/Users/twh/Desktop/validity_App/pipe/primary/theme_keyword_bank.csv\"\n",
    "kw_df        = pd.read_csv(KW_BANK_CSV).dropna(how=\"all\")\n",
    "keyword_bank = {\n",
    "    theme: [str(w).lower() for w in kw_df[theme].dropna()]\n",
    "    for theme in kw_df.columns\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e16476-c30b-4eb5-bd87-699e02ebc363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a comment, I’ll predict VALID (1) or FAKE (0).\n",
      "Then type the correct label [0/1] to log feedback.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  this is the new implementation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twh/miniconda3/envs/proj/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/twh/miniconda3/envs/proj/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ I predict 1 (confidence: 95.5%)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Correct label? [0/1 or Enter to skip]  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twh/miniconda3/envs/proj/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/twh/miniconda3/envs/proj/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged text, prediction, correction, + features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 259\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThen type the correct label [0/1] to log feedback.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m txt:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/proj/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/proj/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# inference_validity.py\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── 1) CONFIG: point these to your saved files ───────────────────────────\n",
    "\n",
    "# Primary‐theme artifacts\n",
    "PRIM_MODEL_PKL  = \"/Users/twh/Desktop/validity_App/pipe/primary/primary_theme_classifier_e5_fixed.pkl\"\n",
    "PRIM_SCALER_PKL = \"/Users/twh/Desktop/validity_App/pipe/primary/handcrafted_scaler.pkl\"\n",
    "LABEL_LE_PKL    = \"/Users/twh/Desktop/validity_App/pipe/primary/primary_theme_label_encoder_embeddings.pkl\"\n",
    "\n",
    "# Secondary‐theme artifacts\n",
    "SEC_MODELS_PKL  = \"secondary_theme_classifier.pkl\"\n",
    "SEC_SCALER_PKL  = \"sec_scaler.pkl\"\n",
    "\n",
    "# Sentiment & emotion pipelines\n",
    "SENT_PKL        = \"/Users/twh/Desktop/validity_App/pipe/sentiment/sentiment_model_with_emoji_vader.pkl\"\n",
    "FULL_EMO_PKL    = \"/Users/twh/Desktop/validity_App/pipe/Emotion/full_emotion_pipeline.pkl\"\n",
    "EMO_LE_PKL      = \"/Users/twh/Desktop/validity_App/pipe/Emotion/emotion_label_encoder_low_neutral.pkl\"\n",
    "\n",
    "# Final validity classifier\n",
    "FINAL_MODEL_PKL = \"validity_classifier.pkl\"\n",
    "\n",
    "# Keyword bank CSV\n",
    "KW_BANK_CSV     = \"/Users/twh/Desktop/validity_App/pipe/primary/theme_keyword_bank.csv\"\n",
    "\n",
    "\n",
    "# ─── 2) LOAD all your artifacts ────────────────────────────────────────────\n",
    "\n",
    "# a) Primary‐theme model & scaler\n",
    "prim_model   = joblib.load(PRIM_MODEL_PKL)\n",
    "prim_scaler  = joblib.load(PRIM_SCALER_PKL)\n",
    "\n",
    "# b) Secondary‐theme models & scaler\n",
    "sec_models   = joblib.load(SEC_MODELS_PKL)  # list of one‐vs‐rest XGBs\n",
    "sec_scaler   = joblib.load(SEC_SCALER_PKL)\n",
    "\n",
    "# c) Sentiment‐fill pipeline\n",
    "sent_pipe    = joblib.load(SENT_PKL)\n",
    "\n",
    "# d) Emotion pipeline\n",
    "emo_full     = joblib.load(FULL_EMO_PKL)\n",
    "emo2_model   = emo_full.emotionality_model\n",
    "emo_model    = emo_full.emotion_model\n",
    "emo_le       = joblib.load(EMO_LE_PKL)\n",
    "ohe_emotion  = OneHotEncoder(\n",
    "    categories=[list(emo_le.classes_)],\n",
    "    sparse_output=False,\n",
    "    handle_unknown=\"ignore\"\n",
    ")\n",
    "# “Prime” the OHE so it knows its categories\n",
    "_ohe_dummy   = ohe_emotion.fit(np.array(emo_le.classes_).reshape(-1,1))\n",
    "\n",
    "# e) Embedder & VADER\n",
    "embedder     = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "vader        = SentimentIntensityAnalyzer()\n",
    "\n",
    "# f) Final validity classifier\n",
    "valid_clf    = joblib.load(FINAL_MODEL_PKL)\n",
    "\n",
    "# g) Keyword bank for text‐stats\n",
    "kw_df        = pd.read_csv(KW_BANK_CSV).dropna(how=\"all\")\n",
    "keyword_bank = {\n",
    "    theme: [str(w).lower() for w in kw_df[theme].dropna()]\n",
    "    for theme in kw_df.columns\n",
    "}\n",
    "\n",
    "\n",
    "# ─── 3) UTILS to compute each block of features ────────────────────────────\n",
    "\n",
    "def get_sentiment(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Run your saved sentiment pipeline on a single‐row DataFrame.\n",
    "    Returns a float in [-1.0, +1.0].\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"original_text\":    [text],\n",
    "        \"english_keywords\": [\"\"],      # no precomputed keywords in inference\n",
    "        \"primary_theme\":    [\"\"],\n",
    "        \"main_emotion\":     [\"\"]\n",
    "    })\n",
    "    score = sent_pipe.predict(df)[0]\n",
    "    # If the output is a string label, map to numeric\n",
    "    if isinstance(score, str):\n",
    "        m = {\"neg\": -1, \"negative\": -1, \"neu\": 0, \"neutral\": 0, \"pos\": 1, \"positive\": 1}\n",
    "        score = m.get(score, 0)\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "def get_main_emotion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    First run the emotionality model (0 or 1). If 0, return \"neutral\".\n",
    "    Otherwise, run the fine‐emotion model and map back via LabelEncoder.\n",
    "    \"\"\"\n",
    "    row = {\n",
    "        \"original_text\":    text,\n",
    "        \"english_keywords\": \"\",\n",
    "        \"primary_theme\":    \"\",\n",
    "        \"sentiment\":        get_sentiment(text)\n",
    "    }\n",
    "    flag = emo2_model.predict(pd.DataFrame([row]))[0]\n",
    "    if flag == 0:\n",
    "        return \"neutral\"\n",
    "    lbl = emo_model.predict(pd.DataFrame([row]))[0]\n",
    "    return emo_le.inverse_transform([lbl])[0]\n",
    "\n",
    "\n",
    "def get_emo_ohe(emotion_label: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One‐hot encode the single emotion label (shape returned = (1, 26)).\n",
    "    \"\"\"\n",
    "    return ohe_emotion.transform([[emotion_label]])\n",
    "\n",
    "\n",
    "def text_stats(text: str, theme: str=\"\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the five handcrafted text‐stats:\n",
    "     1) kw_overlap (words ∩ keyword_bank[theme])\n",
    "     2) length of text\n",
    "     3) has question mark?\n",
    "     4) number of uppercase letters\n",
    "     5) number of exclamation marks\n",
    "    \"\"\"\n",
    "    txt    = text\n",
    "    words  = set(re.findall(r\"\\b[a-z]{3,}\\b\", txt.lower()))\n",
    "    kw_ov  = len(words & set(keyword_bank.get(theme, [])))\n",
    "    return np.array([\n",
    "        kw_ov,\n",
    "        len(txt),\n",
    "        int(\"?\" in txt),\n",
    "        len(re.findall(r\"[A-Z]\", txt)),\n",
    "        len(re.findall(r\"!\", txt))\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "def get_primary_probs(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1) Compute embedding of [text] → shape (1, D_embed)\n",
    "    2) Compute text_stats(text) → shape (1, 5)\n",
    "    3) Scale the 5 stats via prim_scaler → shape (1, 5)\n",
    "    4) hstack: [emb (1×D) ‖ scaled_stats (1×5)] → shape (1, D+5)\n",
    "    5) prim_model.predict_proba(...) → array of length = n_primary_classes\n",
    "    \"\"\"\n",
    "    emb   = embedder.encode([text])                    # (1, D_embed)\n",
    "    stats = text_stats(text).reshape(1, -1)             # (1, 5)\n",
    "    Xp    = np.hstack([emb, prim_scaler.transform(stats)])  # (1, D+5)\n",
    "    return prim_model.predict_proba(Xp)[0]              # (n_primary,)\n",
    "\n",
    "\n",
    "def get_secondary_probs(text: str,\n",
    "                        sentiment: float,\n",
    "                        emo_ohe: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1) emb = embedder.encode([text])            → (1, D_embed)\n",
    "    2) stats = text_stats(text).reshape(1,-1)   → (1, 5)\n",
    "    3) sent_arr = [[sentiment]]                → (1, 1)\n",
    "    4) sec_in = [stats (1×5) ‖ sent_arr (1×1)]  → (1, 6)\n",
    "    5) sec_scaled = sec_scaler.transform(sec_in)   → (1, 6)\n",
    "    6) Xb = [emb (1×D) ‖ sec_scaled (1×6) ‖ emo_ohe (1×26)] → (1, D+6+26)\n",
    "    7) Run each of the sec_models on Xb, return array of their “prob( class = 1 )”\n",
    "       → shape (n_secondary_models,)\n",
    "    \"\"\"\n",
    "    emb      = embedder.encode([text])                # (1, D_embed)\n",
    "    stats    = text_stats(text).reshape(1, -1)         # (1, 5)\n",
    "    sent_arr = np.array([[sentiment]])                 # (1, 1)\n",
    "    sec_in   = np.hstack([stats, sent_arr])            # (1, 6)\n",
    "    sec_s    = sec_scaler.transform(sec_in)            # (1, 6)\n",
    "    Xb       = np.hstack([emb, sec_s, emo_ohe])        # (1, D+6+26)\n",
    "    return np.array([m.predict_proba(Xb)[0, 1] for m in sec_models])  # (n_sec_models,)\n",
    "\n",
    "\n",
    "def infer_validity(text: str) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    1) sentiment = get_sentiment(text)\n",
    "    2) main_emotion = get_main_emotion(text)\n",
    "    3) emo_ohe = get_emo_ohe(main_emotion)       → shape (1,26)\n",
    "    4) pp = get_primary_probs(text).reshape(1,-1) → shape (1, n_primary)\n",
    "    5) sp = get_secondary_probs(text, sentiment, emo_ohe).reshape(1,-1)\n",
    "    6) Xf = [pp (1×n_p) ‖ sp (1×n_s) ‖ emo_ohe (1×26) ‖ [[sentiment]] (1×1)]\n",
    "         → shape (1, total_features)\n",
    "    7) pred = valid_clf.predict(Xf)[0]\n",
    "    8) conf = valid_clf.predict_proba(Xf)[0,1]   # prob “1” = REAL\n",
    "    \"\"\"\n",
    "    s   = get_sentiment(text)\n",
    "    me  = get_main_emotion(text)\n",
    "    eo  = get_emo_ohe(me)                            # (1,26)\n",
    "\n",
    "    pp  = get_primary_probs(text).reshape(1, -1)      # (1, n_primary)\n",
    "    sp  = get_secondary_probs(text, s, eo).reshape(1, -1)  # (1, n_secondary)\n",
    "\n",
    "    s_arr = np.array([[s]])                           # (1,1)\n",
    "    Xf    = np.hstack([pp, sp, eo, s_arr])            # (1, total_features)\n",
    "\n",
    "    pred = valid_clf.predict(Xf)[0]\n",
    "    conf = valid_clf.predict_proba(Xf)[0, 1]\n",
    "    return int(pred), float(conf)\n",
    "\n",
    "\n",
    "def extract_features(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return exactly the 1D feature vector used by infer_validity:\n",
    "      [primary_probs (n_p,) ‖ secondary_probs (n_s,) ‖ emo_onehot (26,) ‖ sentiment (1,)]\n",
    "    \"\"\"\n",
    "    s   = get_sentiment(text)\n",
    "    me  = get_main_emotion(text)\n",
    "    eo  = get_emo_ohe(me).flatten()                  # (26,)\n",
    "    pp  = get_primary_probs(text).flatten()          # (n_p,)\n",
    "    sp  = get_secondary_probs(text, s, eo.reshape(1, -1)).flatten()  # (n_s,)\n",
    "    return np.concatenate([pp, sp, eo, [s]])\n",
    "\n",
    "\n",
    "# ─── 4) FEEDBACK LOGGER ─────────────────────────────────────────────────────\n",
    "\n",
    "def record_feedback(text: str,\n",
    "                    predicted: int,\n",
    "                    correct: int,\n",
    "                    filepath: str=\"feedback.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Compute the feature vector for `text` and append:\n",
    "      [text, predicted, correct, f0, f1, ..., fN]\n",
    "    to `feedback.csv`.  Write header once if file doesn’t exist.\n",
    "    \"\"\"\n",
    "    feats = extract_features(text).tolist()\n",
    "    row   = [text, predicted, correct] + feats\n",
    "\n",
    "    # Write header if the file doesn't exist yet\n",
    "    try:\n",
    "        with open(filepath, 'x', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            header = (\n",
    "                [\"original_text\", \"predicted\", \"correct\"]\n",
    "                + [f\"f{i}\" for i in range(len(feats))]\n",
    "            )\n",
    "            writer.writerow(header)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # Append the data row\n",
    "    with open(filepath, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow(row)\n",
    "\n",
    "\n",
    "# ─── 5) CLI LOOP ───────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Enter a comment, I’ll predict VALID (1) or FAKE (0).\")\n",
    "    print(\"Then type the correct label [0/1] to log feedback.\")\n",
    "    while True:\n",
    "        txt = input(\"> \").strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        pred, conf = infer_validity(txt)\n",
    "        print(f\"→ I predict {pred} (confidence: {conf:.1%})\")\n",
    "\n",
    "        corr = input(\"Correct label? [0/1 or Enter to skip] \").strip()\n",
    "        if corr in (\"0\", \"1\"):\n",
    "            record_feedback(txt, pred, int(corr))\n",
    "            print(\"✅ Logged text, prediction, correction, + features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7552a80-bde1-4e54-83be-35898eaea33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       original_text  predicted  correct  \\\n",
      "0  I still have a Windows XP laptop. I will never...          1        1   \n",
      "1  I still have a Windows XP laptop. I will never...          1        1   \n",
      "2                                 this is my comment          1        1   \n",
      "3                        hello this is a new comment          1        1   \n",
      "\n",
      "   prim_Business  prim_Cryptocurrency  prim_Economy  prim_Entertainment  \\\n",
      "0       0.000112             0.000167      0.000063            0.002496   \n",
      "1       0.000112             0.000167      0.000063            0.002496   \n",
      "2       0.000092             0.000106      0.000295            0.001553   \n",
      "3       0.000151             0.002046      0.000342            0.022179   \n",
      "\n",
      "   prim_Environment  prim_Finance  prim_Health  ...  emo_nervousness  \\\n",
      "0          0.000205      0.000054     0.000256  ...              0.0   \n",
      "1          0.000205      0.000054     0.000256  ...              0.0   \n",
      "2          0.051136      0.000179     0.001109  ...              0.0   \n",
      "3          0.028435      0.000175     0.001754  ...              0.0   \n",
      "\n",
      "   emo_neutral  emo_optimism  emo_pride  emo_realization  emo_relief  \\\n",
      "0          1.0           0.0        0.0              0.0         0.0   \n",
      "1          1.0           0.0        0.0              0.0         0.0   \n",
      "2          1.0           0.0        0.0              0.0         0.0   \n",
      "3          1.0           0.0        0.0              0.0         0.0   \n",
      "\n",
      "   emo_remorse  emo_sadness  emo_surprise  sentiment  \n",
      "0          0.0          0.0           0.0   0.137045  \n",
      "1          0.0          0.0           0.0   0.137045  \n",
      "2          0.0          0.0           0.0   0.037527  \n",
      "3          0.0          0.0           0.0   0.107066  \n",
      "\n",
      "[4 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 1) Load the label encoders or models so we can pull out names:\n",
    "prim_le   = joblib.load(\"/Users/twh/Desktop/Computerscience/CS 482/primary_theme_label_encoder_embeddings.pkl\")\n",
    "sec_models= joblib.load(\"secondary_theme_classifier.pkl\")   # a list of XGBs, one per secondary concept\n",
    "# If you used a label encoder for your secondary themes, load that instead:\n",
    "# sec_le = joblib.load(\"…_secondary_label_encoder.pkl\")\n",
    "\n",
    "emo_le    = joblib.load(\"/Users/twh/Desktop/Computerscience/CS 482/Models/Emotion/emotion_label_encoder_low_neutral.pkl\")\n",
    "\n",
    "# 2) Build the “readable” column‐name lists:\n",
    "#    a) Primary‐theme names:\n",
    "prim_names = [f\"prim_{label}\" for label in prim_le.classes_]\n",
    "\n",
    "#    b) Secondary‐theme names. If you have a sec_le (LabelEncoder) then:\n",
    "# sec_names = [f\"sec_{lbl}\" for lbl in sec_le.classes_]\n",
    "# If you don’t, just index them by 0…len(sec_models)–1:\n",
    "sec_names  = [f\"sec_{i}\" for i in range(len(sec_models))]\n",
    "\n",
    "#    c) Emotion one-hot names:\n",
    "emo_names  = [f\"emo_{lbl}\" for lbl in emo_le.classes_]\n",
    "\n",
    "#    d) Finally, sentiment:\n",
    "sent_name  = [\"sentiment\"]\n",
    "\n",
    "#    e) Combine them all:\n",
    "all_feat_names = prim_names + sec_names + emo_names + sent_name\n",
    "\n",
    "# 3) Read the feedback CSV. The first three columns are: original_text, predicted, correct.\n",
    "df_fb = pd.read_csv(\"feedback.csv\")\n",
    "\n",
    "# 4) Now assign names to the feature columns f0…fN:\n",
    "n_feats = len(all_feat_names)\n",
    "# We expect exactly n_feats = total number of f-columns in feedback.csv\n",
    "assert df_fb.shape[1] == 3 + n_feats, \\\n",
    "       f\"Found {df_fb.shape[1] - 3} f-columns, but expected {n_feats}\"\n",
    "\n",
    "df_fb.columns = [\"original_text\", \"predicted\", \"correct\"] + all_feat_names\n",
    "\n",
    "# 5) Inspect:\n",
    "print(df_fb.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136eeef3-d1cf-4034-8ba9-78ef45a0dd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
